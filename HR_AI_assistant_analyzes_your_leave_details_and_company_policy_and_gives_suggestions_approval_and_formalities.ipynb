{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Gu_EX-sfvBGbeI8RPm-SKVsNUtFr8Ub_",
      "authorship_tag": "ABX9TyN7vqUwTQ4+LWK08VbfTLPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tamilselvan0101/HR_AI_assistant_analyzes_your_leave_details_and_company_policy_and_gives_suggestions_approval/blob/main/HR_AI_assistant_analyzes_your_leave_details_and_company_policy_and_gives_suggestions_approval_and_formalities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-bigquery\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCFQkJzZqsUh",
        "outputId": "4852f5d9-27db-4546-dbb0-c775584f0b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path1= [\"/content/fake_face_book hr policy.pdf\"]\n",
        "path2= \"/content/pizza-412109-78beb3072fb9.json\""
      ],
      "metadata": {
        "id": "gtO4Gfs2yrlY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enter your api key ðŸ‘‡**"
      ],
      "metadata": {
        "id": "2qXpU0huUe5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pls enter your API key HERE\n",
        "APIKEY="
      ],
      "metadata": {
        "id": "ho52soB4T-WF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from re import template\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "employee_id=input(\"enter employee id: \")\n",
        "querry= input(\" enter your query : \")\n",
        "\n",
        "\n",
        "#extratct pdf text\n",
        "conts=path1\n",
        "paragraph =\" \"\n",
        "for path in conts:\n",
        "  pdreader = PdfReader(path)\n",
        "  for i, page in enumerate(pdreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        paragraph += content\n",
        "\n",
        "text_splitter = CharacterTextSplitter(separator = \"\\n\",chunk_size = 1300,chunk_overlap  = 200,length_function = len)\n",
        "texts = text_splitter.split_text(paragraph)\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=APIKEY)\n",
        "storage = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "chain = load_qa_chain(OpenAI(openai_api_key=APIKEY), chain_type=\"stuff\")\n",
        "\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "#key_path= \"/content/hrproject-412815-6f13d1aeb8b5.json\"\n",
        "#project_id=\"hrproject-412815\"\n",
        "\n",
        "key_path = path2\n",
        "project_id = \"pizza-412109\"\n",
        "\n",
        "\n",
        "#bigquery\n",
        "def client_query(key_path,project_id,query):\n",
        "    credentials = service_account.Credentials.from_service_account_file(\n",
        "        key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
        "        )\n",
        "\n",
        "    client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "    query_job = client.query(query)  # Make an API request.\n",
        "    data=query_job.to_dataframe()\n",
        "    #print(\"The query data:\")\n",
        "    return(data)\n",
        "    \"\"\"\n",
        "    for row in query_job:\n",
        "        # Row values can be accessed by field name or index.\n",
        "        print(\"name={}, count={}\".format(row[0], row[\"total_people\"])) \"\"\"\n",
        "\n",
        "\n",
        "fetch_query='''(SELECT * FROM `pizza-412109.pizza123.emp_rem_leave` where employee_id= '{}')'''.format(employee_id)\n",
        "\n",
        "fetch= client_query(key_path,project_id,query=fetch_query)\n",
        "\n",
        "list_of_lists = fetch.values.tolist()\n",
        "\n",
        "print(list_of_lists)\n",
        "nam= list_of_lists [0][2]\n",
        "mail= list_of_lists[0][1]\n",
        "emplid= list_of_lists[0][0]\n",
        "pl_cl= list_of_lists[0][3]\n",
        "sl= list_of_lists[0][4]\n",
        "maternity= list_of_lists[0][5]\n",
        "comp_off= list_of_lists[0][6]\n",
        "carry_forward= list_of_lists[0][7]\n",
        "lop= list_of_lists[0][8]\n",
        "wfh=list_of_lists[0][9]\n",
        "\n",
        "#print(\"value is \")\n",
        "#print(pl_cl)\n",
        "\n",
        "\n",
        "template= (\"i am a employe my name is {} my employee id {} my mail id is {} i have {} pl_cl remaining , {} sick leave remaining , {} maternity leave remaining , {} cam_off remaining , {} leaves carry_forward,{} lop remaining  ,{} wfh  remaining\").format( nam,mail,emplid,pl_cl,sl,maternity,comp_off,carry_forward,lop, wfh )\n",
        "#print(template)\n",
        "\n",
        "\n",
        "#simalirity search\n",
        "def search1(query):\n",
        "    docs = storage.similarity_search(query)\n",
        "    tas=chain.run(input_documents=docs, question=query)\n",
        "    #result_label.config(text=f\"{tas}\")\n",
        "    return(tas)\n",
        "search_reasult=search1(query=querry)\n",
        "\n",
        "#print(search_reasult)\n",
        "\n",
        "\n",
        "#finetuning\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=APIKEY)\n",
        "from langchain.prompts import (ChatPromptTemplate,HumanMessagePromptTemplate,\n",
        "MessagesPlaceholder,SystemMessagePromptTemplate)\n",
        "tass=\"\"\n",
        "\n",
        "infom=(\"You are a nice hr  you will give suggestions and steps by referring the company policy, the compny policy is {} and this information about my self {}  \").format(search_reasult,template)\n",
        "\n",
        "prompt = ChatPromptTemplate(messages=[\n",
        "    SystemMessagePromptTemplate.from_template(infom),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")])\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
        "\n",
        "def search():\n",
        "    query =querry\n",
        "    #docs = storage.similarity_search(query)\n",
        "    tapp = conversation({\"question\": query})\n",
        "    \"\"\"docs[0].page_content += tapp['text']\n",
        "    tass = chain.run(input_documents=docs, question=query)\n",
        "    print(tapp)\n",
        "    \"\"\"\n",
        "    #print(tapp)\n",
        "    tass= tapp.get('text')\n",
        "    print(tass)\n",
        "\n",
        "\n",
        "search()\n",
        "\n",
        "# pls use X/CH/866 for employee id or check the bigquery\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BaqqJ7owmWQ",
        "outputId": "850658b3-2141-408f-b416-626a432948e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter employee id: X/CH/866\n",
            " enter your query : am i eligible for maternity leave\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3683, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1601, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2255, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1397, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2498, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2505, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2037, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2962, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2271, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1682, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1741, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2295, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1628, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1783, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1908, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3054, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3161, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1392, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3796, which is longer than the specified 1300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2249, which is longer than the specified 1300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['X/CH/866', 'gangadhar', 'gangadhar@omail.com', 3, 12, 0, 2, 0, 0, 0]]\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a nice hr  you will give suggestions and steps by referring the company policy, the compny policy is  Based on the context provided, maternity leave is available for all female employees who have completed 80 days of working and have less than two surviving children. Additionally, the employee must provide an approved Medical Practitioner's report confirming their pregnancy and the estimated delivery date, along with their supervisor's approval. It is also important to note that maternity leave can only be taken after 6 months from the completion of the previous maternity leave. If you have any further questions, it is best to consult with your HR Manager for specific eligibility requirements. and this information about my self i am a employe my name is gangadhar@omail.com my employee id gangadhar my mail id is X/CH/866 i have 3 pl_cl remaining , 12 sick leave remaining , 0 maternity leave remaining , 2 cam_off remaining , 0 leaves carry_forward,0 lop remaining  ,0 wfh  remaining  \n",
            "Human: am i eligible for maternity leave\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Based on the company policy provided, in order to be eligible for maternity leave, you need to meet the following criteria:\n",
            "\n",
            "1. You must be a female employee.\n",
            "2. You must have completed 80 days of working.\n",
            "3. You must have less than two surviving children.\n",
            "4. You must provide an approved Medical Practitioner's report confirming your pregnancy and the estimated delivery date.\n",
            "5. You must have your supervisor's approval.\n",
            "\n",
            "Since you have 0 maternity leave remaining, it seems like you have already taken your maternity leave previously. According to the policy, maternity leave can only be taken after 6 months from the completion of the previous maternity leave.\n",
            "\n",
            "If you have any further questions or need more clarification on your eligibility, I recommend consulting with your HR Manager for specific details based on your situation.\n"
          ]
        }
      ]
    }
  ]
}